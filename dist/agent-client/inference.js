/**
 * Inference Client
 *
 * Wraps Anthropic and OpenAI APIs for LLM inference.
 * The automaton pays for its own thinking through direct API keys.
 */
export function createInferenceClient(options) {
    const { openaiApiKey, anthropicApiKey } = options;
    let currentModel = options.defaultModel;
    let maxTokens = options.maxTokens;
    // Validate that at least one inference API key is set
    if (!anthropicApiKey && !openaiApiKey) {
        throw new Error("No inference API key configured. Set anthropicApiKey or openaiApiKey in config.");
    }
    const chat = async (messages, opts) => {
        const model = opts?.model || currentModel;
        const tools = opts?.tools;
        // Newer models (o-series, gpt-5.x, gpt-4.1) require max_completion_tokens
        const usesCompletionTokens = /^(o[1-9]|gpt-5|gpt-4\.1)/.test(model);
        const tokenLimit = opts?.maxTokens || maxTokens;
        const body = {
            model,
            messages: messages.map(formatMessage),
            stream: false,
        };
        if (usesCompletionTokens) {
            body.max_completion_tokens = tokenLimit;
        }
        else {
            body.max_tokens = tokenLimit;
        }
        if (opts?.temperature !== undefined) {
            body.temperature = opts.temperature;
        }
        if (tools && tools.length > 0) {
            body.tools = tools;
            body.tool_choice = "auto";
        }
        const backend = resolveInferenceBackend(model, {
            openaiApiKey,
            anthropicApiKey,
        });
        if (backend === "anthropic") {
            return chatViaAnthropic({
                model,
                tokenLimit,
                messages,
                tools,
                temperature: opts?.temperature,
                anthropicApiKey: anthropicApiKey,
            });
        }
        if (backend !== "openai") {
            throw new Error(`No inference API key configured for model "${model}". Set anthropicApiKey (for claude-* models) or openaiApiKey (for gpt-*/o-series models) in config.`);
        }
        return chatViaOpenAiCompatible({
            model,
            body,
            apiUrl: "https://api.openai.com",
            apiKey: openaiApiKey,
        });
    };
    const setLowComputeMode = (enabled) => {
        if (enabled) {
            currentModel = options.lowComputeModel || "gpt-4.1";
            maxTokens = 4096;
        }
        else {
            currentModel = options.defaultModel;
            maxTokens = options.maxTokens;
        }
    };
    const getDefaultModel = () => {
        return currentModel;
    };
    return {
        chat,
        setLowComputeMode,
        getDefaultModel,
    };
}
function formatMessage(msg) {
    const formatted = {
        role: msg.role,
        content: msg.content,
    };
    if (msg.name)
        formatted.name = msg.name;
    if (msg.tool_calls)
        formatted.tool_calls = msg.tool_calls;
    if (msg.tool_call_id)
        formatted.tool_call_id = msg.tool_call_id;
    return formatted;
}
function resolveInferenceBackend(model, keys) {
    if (keys.anthropicApiKey && /^claude/i.test(model)) {
        return "anthropic";
    }
    if (keys.openaiApiKey && /^(gpt|o[1-9]|chatgpt)/i.test(model)) {
        return "openai";
    }
    return "unknown";
}
async function chatViaOpenAiCompatible(params) {
    const resp = await fetch(`${params.apiUrl}/v1/chat/completions`, {
        method: "POST",
        headers: {
            "Content-Type": "application/json",
            Authorization: `Bearer ${params.apiKey}`,
        },
        body: JSON.stringify(params.body),
    });
    if (!resp.ok) {
        const text = await resp.text();
        throw new Error(`Inference error (openai): ${resp.status}: ${text}`);
    }
    const data = await resp.json();
    const choice = data.choices?.[0];
    if (!choice) {
        throw new Error("No completion choice returned from inference");
    }
    const message = choice.message;
    const usage = {
        promptTokens: data.usage?.prompt_tokens || 0,
        completionTokens: data.usage?.completion_tokens || 0,
        totalTokens: data.usage?.total_tokens || 0,
    };
    const toolCalls = message.tool_calls?.map((tc) => ({
        id: tc.id,
        type: "function",
        function: {
            name: tc.function.name,
            arguments: tc.function.arguments,
        },
    }));
    return {
        id: data.id || "",
        model: data.model || params.model,
        message: {
            role: message.role,
            content: message.content || "",
            tool_calls: toolCalls,
        },
        toolCalls,
        usage,
        finishReason: choice.finish_reason || "stop",
    };
}
async function chatViaAnthropic(params) {
    const transformed = transformMessagesForAnthropic(params.messages);
    const body = {
        model: params.model,
        max_tokens: params.tokenLimit,
        messages: transformed.messages.length > 0
            ? transformed.messages
            : [{ role: "user", content: "Continue." }],
    };
    if (transformed.system) {
        body.system = transformed.system;
    }
    if (params.temperature !== undefined) {
        body.temperature = params.temperature;
    }
    if (params.tools && params.tools.length > 0) {
        body.tools = params.tools.map((tool) => ({
            name: tool.function.name,
            description: tool.function.description,
            input_schema: tool.function.parameters,
        }));
        body.tool_choice = { type: "auto" };
    }
    const resp = await fetch("https://api.anthropic.com/v1/messages", {
        method: "POST",
        headers: {
            "Content-Type": "application/json",
            "x-api-key": params.anthropicApiKey,
            "anthropic-version": "2023-06-01",
        },
        body: JSON.stringify(body),
    });
    if (!resp.ok) {
        const text = await resp.text();
        throw new Error(`Inference error (anthropic): ${resp.status}: ${text}`);
    }
    const data = await resp.json();
    const content = Array.isArray(data.content) ? data.content : [];
    const textBlocks = content.filter((c) => c?.type === "text");
    const toolUseBlocks = content.filter((c) => c?.type === "tool_use");
    const toolCalls = toolUseBlocks.length > 0
        ? toolUseBlocks.map((tool) => ({
            id: tool.id,
            type: "function",
            function: {
                name: tool.name,
                arguments: JSON.stringify(tool.input || {}),
            },
        }))
        : undefined;
    const textContent = textBlocks
        .map((block) => String(block.text || ""))
        .join("\n")
        .trim();
    if (!textContent && !toolCalls?.length) {
        throw new Error("No completion content returned from anthropic inference");
    }
    const promptTokens = data.usage?.input_tokens || 0;
    const completionTokens = data.usage?.output_tokens || 0;
    const usage = {
        promptTokens,
        completionTokens,
        totalTokens: promptTokens + completionTokens,
    };
    return {
        id: data.id || "",
        model: data.model || params.model,
        message: {
            role: "assistant",
            content: textContent,
            tool_calls: toolCalls,
        },
        toolCalls,
        usage,
        finishReason: normalizeAnthropicFinishReason(data.stop_reason),
    };
}
function transformMessagesForAnthropic(messages) {
    const systemParts = [];
    const transformed = [];
    for (const msg of messages) {
        if (msg.role === "system") {
            if (msg.content)
                systemParts.push(msg.content);
            continue;
        }
        if (msg.role === "user") {
            transformed.push({
                role: "user",
                content: msg.content,
            });
            continue;
        }
        if (msg.role === "assistant") {
            const content = [];
            if (msg.content) {
                content.push({ type: "text", text: msg.content });
            }
            for (const toolCall of msg.tool_calls || []) {
                content.push({
                    type: "tool_use",
                    id: toolCall.id,
                    name: toolCall.function.name,
                    input: parseToolArguments(toolCall.function.arguments),
                });
            }
            if (content.length === 0) {
                content.push({ type: "text", text: "" });
            }
            transformed.push({
                role: "assistant",
                content,
            });
            continue;
        }
        if (msg.role === "tool") {
            transformed.push({
                role: "user",
                content: [
                    {
                        type: "tool_result",
                        tool_use_id: msg.tool_call_id || "unknown_tool_call",
                        content: msg.content,
                    },
                ],
            });
        }
    }
    return {
        system: systemParts.length > 0 ? systemParts.join("\n\n") : undefined,
        messages: transformed,
    };
}
function parseToolArguments(raw) {
    try {
        const parsed = JSON.parse(raw);
        if (parsed && typeof parsed === "object" && !Array.isArray(parsed)) {
            return parsed;
        }
        return { value: parsed };
    }
    catch {
        return { _raw: raw };
    }
}
function normalizeAnthropicFinishReason(reason) {
    if (typeof reason !== "string")
        return "stop";
    if (reason === "tool_use")
        return "tool_calls";
    return reason;
}
//# sourceMappingURL=inference.js.map